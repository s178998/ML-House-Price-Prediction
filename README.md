# ğŸ¡ California House Price Prediction â€” Linear Regression Deep Dive

A complete machine learning project to predict **median house values in California** using **deep learning (TensorFlow)** and **regularization models (Ridge, Lasso)**.

---

## ğŸš€ What This Model Does

This model predicts housing prices using a real-world dataset: **California Housing** from `sklearn.datasets`. It uses both:

- A custom-built **neural network** trained with TensorFlow/Keras
- Regularized regression models (**Ridge**, **Lasso**) for comparison

---

## ğŸ§° Tools Used

| Purpose              | Library                |
|----------------------|------------------------|
| Deep learning        | TensorFlow / Keras     |
| Data manipulation    | Pandas, NumPy          |
| Visualization        | Matplotlib, Seaborn    |
| ML models & metrics  | Scikit-learn (Ridge, Lasso, GridSearchCV, RÂ², MAE, MSE) |

---

## ğŸ“Š Model Results (Neural Network)

| Metric       | Value    |
|--------------|----------|
| RÂ² Score     | ~0.85    |
| MAE          | ~0.30    |
| MSE          | ~0.21    |
| RMSE         | ~0.46    |

> âš ï¸ Model is **still being tuned** â€” early stopping and dropout are implemented to reduce overfitting.

---

## ğŸ§  Deep Learning Architecture

```python
tf.keras.Sequential([
    tf.keras.layers.Input(shape=(n_features,)),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.3),
    
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.2),
    
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.1),

    tf.keras.layers.Dense(1)  # Output layer for regression
])


ğŸ“Œ Key Things I Learned
How to use BatchNormalization, Dropout, and EarlyStopping to prevent overfitting.

How to fine-tune traditional models using GridSearchCV.

How important feature scaling and coefficient interpretation are.

That an RÂ² of 1 isn't always good â€” it's likely overfitting.


